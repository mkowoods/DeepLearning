{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implementation of assignment and solutions to problems: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/2_fullyconnected.ipynb\n",
    "\n",
    "Assignment 2\n",
    "---------\n",
    "\n",
    "Previously in 1_notmnist.ipynb, we created a pickle with formatted datasets for training, development and testing on the notMNIST dataset.\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PKL_FILE = 'notMNIST.pickle'\n",
    "IMG_WIDTH = 28\n",
    "IMG_HEIGHT = 28\n",
    "IMG_PIXELS = IMG_HEIGHT * IMG_WIDTH\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (200000, 28, 28) (200000,)\n",
      "Test (10000, 28, 28) (10000,)\n",
      "Validation (10000, 28, 28) (10000,)\n",
      "Training (200000, 784) (200000, 10)\n",
      "Test (10000, 784) (10000, 10)\n",
      "Validation (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(PKL_FILE, 'rb') as f:\n",
    "    save_file = pickle.load(f)\n",
    "    train_dataset = save_file['train_dataset']\n",
    "    train_labels  = save_file['train_labels']\n",
    "    test_dataset  = save_file['test_dataset']\n",
    "    test_labels   = save_file['test_labels']\n",
    "    val_dataset   = save_file['valid_dataset']\n",
    "    val_labels    = save_file['valid_labels']\n",
    "    \n",
    "    del save_file\n",
    "    \n",
    "print 'Training', train_dataset.shape, train_labels.shape\n",
    "print 'Test', test_dataset.shape, test_labels.shape\n",
    "print 'Validation', val_dataset.shape, val_labels.shape\n",
    "\n",
    "def reformat_dataset(dataset):\n",
    "    return dataset.reshape(-1, IMG_HEIGHT * IMG_WIDTH).astype(np.float32)\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    if(len(labels.shape) > 1):\n",
    "        raise Exception('labels should be a vector')\n",
    "    return (np.arange(NUM_CLASSES) == labels.reshape(-1, 1)).astype(np.float32)\n",
    "\n",
    "train_dataset = reformat_dataset(train_dataset)\n",
    "train_labels  = one_hot_encode(train_labels)\n",
    "test_dataset  = reformat_dataset(test_dataset)\n",
    "test_labels   = one_hot_encode(test_labels)\n",
    "val_dataset   = reformat_dataset(val_dataset)\n",
    "val_labels    = one_hot_encode(val_labels)\n",
    "\n",
    "print 'Training', train_dataset.shape, train_labels.shape\n",
    "print 'Test', test_dataset.shape, test_labels.shape\n",
    "print 'Validation', val_dataset.shape, val_labels.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Logistic Regression\n",
    "-----------\n",
    "\n",
    "We're first going to train a multinomial logistic regression using simple gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_subset = 10000\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_val_dataset = tf.constant(val_dataset)\n",
    "    \n",
    "    #weights matrix\n",
    "    #initialized as a set of random results \n",
    "    #train weights to predict the log probability for each of the clases based on the\n",
    "    #for each class - sigmoid(w1*x1 + w2*x2 + .... + wN * xN + b)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal( shape = (IMG_PIXELS, NUM_CLASSES)))\n",
    "    biases = tf.Variable(tf.zeros([NUM_CLASSES]))\n",
    "    \n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(labels = tf_train_labels, logits = logits)) \n",
    "    opt = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    train_predictions = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_val_dataset, weights) + biases)\n",
    "    test_prediction  = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step:  0 17.1443 Train Acc 0.0993 Valid Acc 0.1273\n",
      "Loss at step:  50 2.9168 Train Acc 0.6562 Valid Acc 0.6463\n",
      "Loss at step:  100 2.31434 Train Acc 0.7112 Valid Acc 0.6944\n",
      "Loss at step:  150 2.02989 Train Acc 0.7329 Valid Acc 0.7144\n",
      "Loss at step:  200 1.84387 Train Acc 0.7448 Valid Acc 0.7258\n",
      "Loss at step:  250 1.70552 Train Acc 0.7539 Valid Acc 0.7318\n",
      "Loss at step:  300 1.59633 Train Acc 0.7601 Valid Acc 0.7355\n",
      "Loss at step:  350 1.50721 Train Acc 0.7666 Valid Acc 0.738\n",
      "Loss at step:  400 1.43254 Train Acc 0.7731 Valid Acc 0.7388\n",
      "Loss at step:  450 1.36861 Train Acc 0.7767 Valid Acc 0.7406\n",
      "Loss at step:  500 1.31293 Train Acc 0.7792 Valid Acc 0.7426\n",
      "Loss at step:  550 1.26375 Train Acc 0.7819 Valid Acc 0.7435\n",
      "Loss at step:  600 1.21982 Train Acc 0.7837 Valid Acc 0.7448\n",
      "Loss at step:  650 1.1802 Train Acc 0.7876 Valid Acc 0.7468\n",
      "Loss at step:  700 1.1442 Train Acc 0.7908 Valid Acc 0.7483\n",
      "Loss at step:  750 1.11128 Train Acc 0.7925 Valid Acc 0.7491\n",
      "Loss at step:  800 1.08099 Train Acc 0.7945 Valid Acc 0.7498\n",
      "Test Accuracy 0.8262\n",
      "Runtime 51.1197490692\n"
     ]
    }
   ],
   "source": [
    "training_steps = 801\n",
    "\n",
    "acc = lambda preds, labels : np.sum(np.argmax(preds, 1) == np.argmax(labels, 1))/float(preds.shape[0])\n",
    "training_acc = []\n",
    "validation_acc = []\n",
    "start = time.time()\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    for step in range(training_steps):\n",
    "        _, loss_score, pred = sess.run([opt, loss, train_predictions])\n",
    "        train_acc = acc(pred, tf_train_labels.eval())\n",
    "        val_acc = acc(valid_prediction.eval(), val_labels)\n",
    "        training_acc.append(train_acc)\n",
    "        validation_acc.append(val_acc)\n",
    "        if step % 50 == 0:\n",
    "            print 'Loss at step: ', step, loss_score,\n",
    "            print 'Train Acc', train_acc, #biased\n",
    "            print 'Valid Acc', val_acc\n",
    "    print 'Test Accuracy', acc(test_prediction.eval(), test_labels)\n",
    "print 'Runtime', time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "minibatch_graph = tf.Graph()\n",
    "with minibatch_graph.as_default():\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, IMG_PIXELS))\n",
    "    tf_train_labels  = tf.placeholder(tf.float32, shape = (batch_size, NUM_CLASSES))\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_val_dataset = tf.constant(val_dataset)\n",
    "    \n",
    "    #weights matrix\n",
    "    #initialized as a set of random results \n",
    "    #train weights to predict the log probability for each of the clases based on the\n",
    "    #for each class - sigmoid(w1*x1 + w2*x2 + .... + wN * xN + b)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal( shape = (IMG_PIXELS, NUM_CLASSES)))\n",
    "    biases = tf.Variable(tf.zeros([NUM_CLASSES]))\n",
    "    \n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(labels = tf_train_labels, logits = logits)) \n",
    "    opt = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    train_predictions = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_val_dataset, weights) + biases)\n",
    "    test_prediction  = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step:  0 17.2023 Train Acc 0.0625 Valid Acc 0.0948\n",
      "Loss at step:  50 2.75855 Train Acc 0.6484375 Valid Acc 0.6616\n",
      "Loss at step:  100 1.95608 Train Acc 0.6796875 Valid Acc 0.7143\n",
      "Loss at step:  150 2.23437 Train Acc 0.734375 Valid Acc 0.7322\n",
      "Loss at step:  200 2.07224 Train Acc 0.7578125 Valid Acc 0.7398\n",
      "Loss at step:  250 2.35143 Train Acc 0.7109375 Valid Acc 0.7443\n",
      "Loss at step:  300 1.41116 Train Acc 0.8515625 Valid Acc 0.7476\n",
      "Loss at step:  350 1.8085 Train Acc 0.71875 Valid Acc 0.7538\n",
      "Loss at step:  400 1.44126 Train Acc 0.765625 Valid Acc 0.753\n",
      "Loss at step:  450 1.31516 Train Acc 0.734375 Valid Acc 0.7564\n",
      "Loss at step:  500 1.85536 Train Acc 0.703125 Valid Acc 0.7555\n",
      "Loss at step:  550 2.07593 Train Acc 0.7109375 Valid Acc 0.7583\n",
      "Loss at step:  600 1.09346 Train Acc 0.7890625 Valid Acc 0.7545\n",
      "Loss at step:  650 1.52374 Train Acc 0.7265625 Valid Acc 0.7601\n",
      "Loss at step:  700 1.8009 Train Acc 0.6796875 Valid Acc 0.7582\n",
      "Loss at step:  750 1.61678 Train Acc 0.7421875 Valid Acc 0.7631\n",
      "Loss at step:  800 1.25347 Train Acc 0.78125 Valid Acc 0.7604\n",
      "Loss at step:  850 1.45106 Train Acc 0.78125 Valid Acc 0.7612\n",
      "Loss at step:  900 1.38049 Train Acc 0.7421875 Valid Acc 0.7687\n",
      "Loss at step:  950 1.48673 Train Acc 0.765625 Valid Acc 0.7662\n",
      "Loss at step:  1000 1.37533 Train Acc 0.7421875 Valid Acc 0.7639\n",
      "Loss at step:  1050 1.40043 Train Acc 0.796875 Valid Acc 0.7629\n",
      "Loss at step:  1100 1.25033 Train Acc 0.734375 Valid Acc 0.7621\n",
      "Loss at step:  1150 1.88512 Train Acc 0.671875 Valid Acc 0.7648\n",
      "Loss at step:  1200 1.01018 Train Acc 0.78125 Valid Acc 0.7631\n",
      "Loss at step:  1250 1.31327 Train Acc 0.8125 Valid Acc 0.7661\n",
      "Loss at step:  1300 1.01719 Train Acc 0.8125 Valid Acc 0.7684\n",
      "Loss at step:  1350 0.879993 Train Acc 0.859375 Valid Acc 0.7593\n",
      "Loss at step:  1400 1.24594 Train Acc 0.796875 Valid Acc 0.7672\n",
      "Loss at step:  1450 1.02217 Train Acc 0.7890625 Valid Acc 0.763\n",
      "Loss at step:  1500 1.46347 Train Acc 0.7421875 Valid Acc 0.7687\n",
      "Test Accuracy 0.8451\n",
      "Runtime 30.4462749958\n"
     ]
    }
   ],
   "source": [
    "training_steps = 1501\n",
    "train_dataset_size = train_dataset.shape[0]\n",
    "\n",
    "training_hist = []\n",
    "validation_hist = []\n",
    "loss_hist = []\n",
    "start = time.time()\n",
    "with tf.Session(graph = minibatch_graph) as sess:\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    for step in range(training_steps):\n",
    "        sample = np.random.choice(np.arange(train_dataset_size), 128)\n",
    "        data_batch = train_dataset[sample]\n",
    "        label_batch = train_labels[sample]\n",
    "        _, loss_score, pred = sess.run([opt, loss, train_predictions], feed_dict = {tf_train_dataset : data_batch,\n",
    "                                                                                    tf_train_labels  : label_batch\n",
    "                                                                                   })\n",
    "        train_acc = acc(pred, label_batch)\n",
    "        val_acc = acc(valid_prediction.eval(), val_labels)\n",
    "        training_acc.append(train_acc)\n",
    "        validation_acc.append(val_acc)\n",
    "        loss_hist.append(loss_score)\n",
    "        if step % 50 == 0:\n",
    "            print 'Loss at step: ', step, loss_score,\n",
    "            print 'Train Acc', train_acc, #biased\n",
    "            print 'Valid Acc', val_acc\n",
    "    print 'Test Accuracy', acc(test_prediction.eval(), test_labels)\n",
    "print 'Runtime', time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "Problem\n",
    "-------\n",
    "Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units nn.relu() and 1024 hidden nodes. This model should improve your validation / test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "minibatch_graph = tf.Graph()\n",
    "with minibatch_graph.as_default():\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (None, IMG_PIXELS))\n",
    "    tf_train_labels  = tf.placeholder(tf.float32, shape = (None, NUM_CLASSES))\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_val_dataset = tf.constant(val_dataset)\n",
    "    \n",
    "    #weights matrix\n",
    "    #initialized as a set of random results \n",
    "    #train weights to predict the log probability for each of the clases based on the\n",
    "    #for each class - sigmoid(w1*x1 + w2*x2 + .... + wN * xN + b)\n",
    "    \n",
    "    w1 = tf.get_variable(\"W1\", shape = (IMG_PIXELS, 1024), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.truncated_normal(shape = [1024], mean = 0.1, stddev=0.05))\n",
    "    #w1 = tf.Variable(tf.truncated_normal( shape = (IMG_PIXELS, 1024)))\n",
    "    #b1 = tf.Variable(tf.zeros([1024]))\n",
    "    h1 = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1)\n",
    "    \n",
    "    w2 = tf.get_variable(\"W2\", shape = (1024, NUM_CLASSES), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.truncated_normal(shape = [NUM_CLASSES], mean = 0.1, stddev=0.05))\n",
    "    #w2 = tf.Variable(tf.truncated_normal( shape = (1024, NUM_CLASSES)))\n",
    "    #b2 = tf.Variable(tf.zeros([NUM_CLASSES]))\n",
    "    logits = tf.matmul(h1, w2) + b2\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(labels = tf_train_labels, logits = logits)) \n",
    "    opt = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "    \n",
    "    predictions = tf.nn.softmax(logits)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step:  0 2.43762 Train Acc 0.90625 Valid Acc 0.8645\n",
      "Elapsed Time 0.237092018127\n",
      "Loss at step:  50 0.803512 Train Acc 0.0625 Valid Acc 0.3158\n",
      "Elapsed Time 1.55342411995\n",
      "Loss at step:  100 0.706652 Train Acc 0.78125 Valid Acc 0.8062\n",
      "Elapsed Time 2.83118104935\n",
      "Loss at step:  150 0.494457 Train Acc 0.8046875 Valid Acc 0.8151\n",
      "Elapsed Time 4.11843705177\n",
      "Loss at step:  200 0.666156 Train Acc 0.8515625 Valid Acc 0.823\n",
      "Elapsed Time 5.42649316788\n",
      "Loss at step:  250 0.631942 Train Acc 0.796875 Valid Acc 0.8263\n",
      "Elapsed Time 6.69212317467\n",
      "Loss at step:  300 0.596063 Train Acc 0.8359375 Valid Acc 0.8302\n",
      "Elapsed Time 7.99749612808\n",
      "Loss at step:  350 0.515667 Train Acc 0.8359375 Valid Acc 0.8283\n",
      "Elapsed Time 9.24721097946\n",
      "Loss at step:  400 0.407151 Train Acc 0.859375 Valid Acc 0.8376\n",
      "Elapsed Time 10.533064127\n",
      "Loss at step:  450 0.586637 Train Acc 0.8984375 Valid Acc 0.8365\n",
      "Elapsed Time 11.8072650433\n",
      "Loss at step:  500 0.555998 Train Acc 0.859375 Valid Acc 0.8384\n",
      "Elapsed Time 13.1273970604\n",
      "Loss at step:  550 0.469337 Train Acc 0.8828125 Valid Acc 0.8409\n",
      "Elapsed Time 14.4163680077\n",
      "Loss at step:  600 0.519083 Train Acc 0.8671875 Valid Acc 0.8418\n",
      "Elapsed Time 15.7433650494\n",
      "Loss at step:  650 0.521713 Train Acc 0.8515625 Valid Acc 0.8402\n",
      "Elapsed Time 17.0328409672\n",
      "Loss at step:  700 0.398563 Train Acc 0.8671875 Valid Acc 0.841\n",
      "Elapsed Time 18.3453571796\n",
      "Loss at step:  750 0.344008 Train Acc 0.875 Valid Acc 0.8466\n",
      "Elapsed Time 19.6536021233\n",
      "Loss at step:  800 0.544249 Train Acc 0.90625 Valid Acc 0.8414\n",
      "Elapsed Time 20.9628419876\n",
      "Loss at step:  850 0.618558 Train Acc 0.8359375 Valid Acc 0.8455\n",
      "Elapsed Time 22.3204641342\n",
      "Loss at step:  900 0.414271 Train Acc 0.8046875 Valid Acc 0.8449\n",
      "Elapsed Time 23.6811020374\n",
      "Loss at step:  950 0.530097 Train Acc 0.8984375 Valid Acc 0.8442\n",
      "Elapsed Time 25.0397210121\n",
      "Loss at step:  1000 0.410588 Train Acc 0.8515625 Valid Acc 0.8509\n",
      "Elapsed Time 26.3988001347\n",
      "Loss at step:  1050 0.582949 Train Acc 0.8984375 Valid Acc 0.8527\n",
      "Elapsed Time 27.8503260612\n",
      "Loss at step:  1100 0.634435 Train Acc 0.828125 Valid Acc 0.8527\n",
      "Elapsed Time 29.2834560871\n",
      "Loss at step:  1150 0.487168 Train Acc 0.828125 Valid Acc 0.8531\n",
      "Elapsed Time 30.7046060562\n",
      "Loss at step:  1200 0.41356 Train Acc 0.8515625 Valid Acc 0.8541\n",
      "Elapsed Time 32.116230011\n",
      "Loss at step:  1250 0.496412 Train Acc 0.875 Valid Acc 0.855\n",
      "Elapsed Time 33.4668390751\n",
      "Loss at step:  1300 0.396472 Train Acc 0.8515625 Valid Acc 0.8555\n",
      "Elapsed Time 34.8012621403\n",
      "Loss at step:  1350 0.346294 Train Acc 0.8671875 Valid Acc 0.8536\n",
      "Elapsed Time 36.1649701595\n",
      "Loss at step:  1400 0.360983 Train Acc 0.890625 Valid Acc 0.8565\n",
      "Elapsed Time 37.5224671364\n",
      "Loss at step:  1450 0.428532 Train Acc 0.9140625 Valid Acc 0.8565\n",
      "Elapsed Time 38.8951501846\n",
      "Loss at step:  1500 0.335141 Train Acc 0.8671875 Valid Acc 0.8579\n",
      "Elapsed Time 40.3149330616\n",
      "Loss at step:  1550 0.423306 Train Acc 0.8984375 Valid Acc 0.8552\n",
      "Elapsed Time 41.7008941174\n",
      "Loss at step:  1600 0.502542 Train Acc 0.8828125 Valid Acc 0.8584\n",
      "Elapsed Time 43.0666251183\n",
      "Loss at step:  1650 0.298398 Train Acc 0.8515625 Valid Acc 0.8609\n",
      "Elapsed Time 44.4032270908\n",
      "Loss at step:  1700 0.372418 Train Acc 0.9140625 Valid Acc 0.8613\n",
      "Elapsed Time 45.7538900375\n",
      "Loss at step:  1750 0.511643 Train Acc 0.90625 Valid Acc 0.8594\n",
      "Elapsed Time 47.1930069923\n",
      "Loss at step:  1800 0.479442 Train Acc 0.859375 Valid Acc 0.861\n",
      "Elapsed Time 48.5510179996\n",
      "Loss at step:  1850 0.372079 Train Acc 0.8671875 Valid Acc 0.8607\n",
      "Elapsed Time 49.8920910358\n",
      "Loss at step:  1900 0.458658 Train Acc 0.8984375 Valid Acc 0.865\n",
      "Elapsed Time 51.2325601578\n",
      "Loss at step:  1950 0.210558 Train Acc 0.8671875 Valid Acc 0.8661\n",
      "Elapsed Time 52.5998439789\n",
      "Loss at step:  2000 0.312774 Train Acc 0.9375 Valid Acc 0.8662\n",
      "Elapsed Time 53.9216091633\n",
      "Loss at step:  2050 0.341149 Train Acc 0.90625 Valid Acc 0.8649\n",
      "Elapsed Time 55.2945361137\n",
      "Loss at step:  2100 0.331535 Train Acc 0.9375 Valid Acc 0.8656\n",
      "Elapsed Time 56.6672260761\n",
      "Loss at step:  2150 0.559231 Train Acc 0.8984375 Valid Acc 0.866\n",
      "Elapsed Time 58.0766689777\n",
      "Loss at step:  2200 0.410044 Train Acc 0.828125 Valid Acc 0.8644\n",
      "Elapsed Time 59.496981144\n",
      "Loss at step:  2250 0.402751 Train Acc 0.875 Valid Acc 0.8664\n",
      "Elapsed Time 60.8748571873\n",
      "Loss at step:  2300 0.362845 Train Acc 0.8984375 Valid Acc 0.8688\n",
      "Elapsed Time 62.2411961555\n",
      "Loss at step:  2350 0.24524 Train Acc 0.890625 Valid Acc 0.8662\n",
      "Elapsed Time 63.6249961853\n",
      "Loss at step:  2400 0.483521 Train Acc 0.9296875 Valid Acc 0.867\n",
      "Elapsed Time 64.9964079857\n",
      "Loss at step:  2450 0.348842 Train Acc 0.875 Valid Acc 0.8717\n",
      "Elapsed Time 66.4222359657\n",
      "Loss at step:  2500 0.424209 Train Acc 0.90625 Valid Acc 0.87\n",
      "Elapsed Time 67.8037650585\n",
      "Loss at step:  2550 0.419604 Train Acc 0.890625 Valid Acc 0.8691\n",
      "Elapsed Time 69.1369681358\n",
      "Loss at step:  2600 0.300799 Train Acc 0.8984375 Valid Acc 0.8691\n",
      "Elapsed Time 70.4956829548\n",
      "Loss at step:  2650 0.371275 Train Acc 0.9140625 Valid Acc 0.8719\n",
      "Elapsed Time 71.8387041092\n",
      "Loss at step:  2700 0.338814 Train Acc 0.890625 Valid Acc 0.873\n",
      "Elapsed Time 73.2285161018\n",
      "Loss at step:  2750 0.434058 Train Acc 0.875 Valid Acc 0.8712\n",
      "Elapsed Time 74.6263651848\n",
      "Loss at step:  2800 0.449588 Train Acc 0.8671875 Valid Acc 0.8727\n",
      "Elapsed Time 75.9890589714\n",
      "Loss at step:  2850 0.434426 Train Acc 0.859375 Valid Acc 0.8744\n",
      "Elapsed Time 77.3651649952\n",
      "Loss at step:  2900 0.274564 Train Acc 0.890625 Valid Acc 0.8719\n",
      "Elapsed Time 78.7467610836\n",
      "Loss at step:  2950 0.389586 Train Acc 0.921875 Valid Acc 0.8709\n",
      "Elapsed Time 80.1110460758\n",
      "Loss at step:  3000 0.399565 Train Acc 0.9140625 Valid Acc 0.8711\n",
      "Elapsed Time 81.483494997\n",
      "Loss at step:  3050 0.313677 Train Acc 0.8984375 Valid Acc 0.8733\n",
      "Elapsed Time 82.8276591301\n",
      "Loss at step:  3100 0.466116 Train Acc 0.9296875 Valid Acc 0.8733\n",
      "Elapsed Time 84.2251591682\n",
      "Loss at step:  3150 0.373968 Train Acc 0.84375 Valid Acc 0.8748\n",
      "Elapsed Time 85.5746150017\n",
      "Loss at step:  3200 0.510932 Train Acc 0.890625 Valid Acc 0.8733\n",
      "Elapsed Time 86.9349160194\n",
      "Loss at step:  3250 0.29036 Train Acc 0.8828125 Valid Acc 0.8752\n",
      "Elapsed Time 88.305106163\n",
      "Loss at step:  3300 0.494635 Train Acc 0.90625 Valid Acc 0.8759\n",
      "Elapsed Time 89.642580986\n",
      "Loss at step:  3350 0.405594 Train Acc 0.859375 Valid Acc 0.8753\n",
      "Elapsed Time 90.9945909977\n",
      "Loss at step:  3400 0.38213 Train Acc 0.90625 Valid Acc 0.8761\n",
      "Elapsed Time 92.3795640469\n",
      "Loss at step:  3450 0.455465 Train Acc 0.8671875 Valid Acc 0.8762\n",
      "Elapsed Time 93.7359580994\n",
      "Test Accuracy 0.9371\n",
      "Runtime 95.9152300358\n"
     ]
    }
   ],
   "source": [
    "training_steps = 3500\n",
    "train_dataset_size = train_dataset.shape[0]\n",
    "\n",
    "training_hist = []\n",
    "validation_hist = []\n",
    "loss_hist = []\n",
    "start = time.time()\n",
    "with tf.Session(graph = minibatch_graph) as sess:\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    for step in range(training_steps):\n",
    "        sample = np.random.choice(np.arange(train_dataset_size), 128)\n",
    "        data_batch = train_dataset[sample]\n",
    "        label_batch = train_labels[sample]\n",
    "        _, loss_score, pred = sess.run([opt, loss, predictions], feed_dict = {tf_train_dataset : data_batch,\n",
    "                                                                                    tf_train_labels  : label_batch\n",
    "                                                                                   })\n",
    "        if step % 50 == 0:\n",
    "            print 'Loss at step: ', step, loss_score,\n",
    "            print 'Train Acc', train_acc, #biased\n",
    "            print 'Valid Acc', val_acc\n",
    "            print 'Elapsed Time', time.time() - start\n",
    "            \n",
    "            #log results\n",
    "            valid_pred = sess.run(predictions, feed_dict = {tf_train_dataset : val_dataset})\n",
    "            train_acc = acc(pred, label_batch)\n",
    "            val_acc = acc(valid_pred, val_labels)\n",
    "            training_hist.append(train_acc)\n",
    "            validation_hist.append(val_acc)\n",
    "            loss_hist.append(loss_score)\n",
    "        \n",
    "    test_pred = sess.run(predictions, feed_dict = {tf_train_dataset : test_dataset})    \n",
    "    print 'Test Accuracy', acc(test_pred, test_labels)\n",
    "print 'Runtime', time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJyKKIQzqREQxhEGdiCiGMKgTEcWQ/w9vSUAEDZ+hMQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122d12910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_hist)\n",
    "plt.plot(validation_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "Same Architecture as above using Adam with default settings\n",
    "--------\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step:  0 2.37981 Train Acc 0.7265625 Valid Acc 0.6896\n",
      "Elapsed Time 0.265480041504\n",
      "Loss at step:  50 0.566345 Train Acc 0.125 Valid Acc 0.4657\n",
      "Elapsed Time 1.61279296875\n",
      "Loss at step:  100 0.482214 Train Acc 0.84375 Valid Acc 0.8183\n",
      "Elapsed Time 2.94011211395\n",
      "Loss at step:  150 0.705916 Train Acc 0.8359375 Valid Acc 0.8265\n",
      "Elapsed Time 4.31871700287\n",
      "Loss at step:  200 0.508679 Train Acc 0.8125 Valid Acc 0.8355\n",
      "Elapsed Time 5.74705004692\n",
      "Loss at step:  250 0.533054 Train Acc 0.875 Valid Acc 0.8359\n",
      "Elapsed Time 7.3564221859\n",
      "Loss at step:  300 0.44953 Train Acc 0.8515625 Valid Acc 0.8386\n",
      "Elapsed Time 8.77124714851\n",
      "Loss at step:  350 0.37928 Train Acc 0.859375 Valid Acc 0.8483\n",
      "Elapsed Time 10.161080122\n",
      "Loss at step:  400 0.385821 Train Acc 0.8671875 Valid Acc 0.8377\n",
      "Elapsed Time 11.5672860146\n",
      "Loss at step:  450 0.631951 Train Acc 0.8828125 Valid Acc 0.8504\n",
      "Elapsed Time 13.1315760612\n",
      "Loss at step:  500 0.549704 Train Acc 0.8125 Valid Acc 0.8534\n",
      "Elapsed Time 14.5584771633\n",
      "Loss at step:  550 0.545869 Train Acc 0.859375 Valid Acc 0.854\n",
      "Elapsed Time 16.0017340183\n",
      "Loss at step:  600 0.452667 Train Acc 0.84375 Valid Acc 0.8545\n",
      "Elapsed Time 17.6256320477\n",
      "Loss at step:  650 0.383826 Train Acc 0.8671875 Valid Acc 0.8556\n",
      "Elapsed Time 19.1585850716\n",
      "Loss at step:  700 0.4292 Train Acc 0.890625 Valid Acc 0.8611\n",
      "Elapsed Time 20.8863031864\n",
      "Loss at step:  750 0.414361 Train Acc 0.8515625 Valid Acc 0.8554\n",
      "Elapsed Time 22.7123651505\n",
      "Loss at step:  800 0.338139 Train Acc 0.890625 Valid Acc 0.858\n",
      "Elapsed Time 24.5551860332\n",
      "Loss at step:  850 0.395697 Train Acc 0.90625 Valid Acc 0.8599\n",
      "Elapsed Time 26.0675060749\n",
      "Loss at step:  900 0.374947 Train Acc 0.859375 Valid Acc 0.8646\n",
      "Elapsed Time 28.3128781319\n",
      "Loss at step:  950 0.410755 Train Acc 0.890625 Valid Acc 0.8624\n",
      "Elapsed Time 30.2442550659\n",
      "Loss at step:  1000 0.447347 Train Acc 0.875 Valid Acc 0.8609\n",
      "Elapsed Time 32.1901540756\n",
      "Loss at step:  1050 0.361199 Train Acc 0.8828125 Valid Acc 0.8676\n",
      "Elapsed Time 33.9869501591\n",
      "Loss at step:  1100 0.547237 Train Acc 0.8828125 Valid Acc 0.8659\n",
      "Elapsed Time 35.7629060745\n",
      "Loss at step:  1150 0.28662 Train Acc 0.8671875 Valid Acc 0.8665\n",
      "Elapsed Time 37.4349710941\n",
      "Loss at step:  1200 0.242139 Train Acc 0.9140625 Valid Acc 0.8687\n",
      "Elapsed Time 39.1793839931\n",
      "Loss at step:  1250 0.272155 Train Acc 0.9296875 Valid Acc 0.8708\n",
      "Elapsed Time 40.7440450191\n",
      "Loss at step:  1300 0.430445 Train Acc 0.921875 Valid Acc 0.8709\n",
      "Elapsed Time 42.2231111526\n",
      "Loss at step:  1350 0.44432 Train Acc 0.84375 Valid Acc 0.8691\n",
      "Elapsed Time 43.7003331184\n",
      "Loss at step:  1400 0.288168 Train Acc 0.875 Valid Acc 0.8692\n",
      "Elapsed Time 45.2035381794\n",
      "Loss at step:  1450 0.434976 Train Acc 0.8984375 Valid Acc 0.8643\n",
      "Elapsed Time 46.7035191059\n",
      "Loss at step:  1500 0.503022 Train Acc 0.875 Valid Acc 0.8741\n",
      "Elapsed Time 48.2106211185\n",
      "Loss at step:  1550 0.332936 Train Acc 0.875 Valid Acc 0.8701\n",
      "Elapsed Time 49.7064960003\n",
      "Loss at step:  1600 0.392304 Train Acc 0.875 Valid Acc 0.8733\n",
      "Elapsed Time 51.1717300415\n",
      "Loss at step:  1650 0.319287 Train Acc 0.859375 Valid Acc 0.8755\n",
      "Elapsed Time 52.5897021294\n",
      "Loss at step:  1700 0.311696 Train Acc 0.921875 Valid Acc 0.8741\n",
      "Elapsed Time 54.0042691231\n",
      "Loss at step:  1750 0.27712 Train Acc 0.90625 Valid Acc 0.8732\n",
      "Elapsed Time 55.7920181751\n",
      "Loss at step:  1800 0.371539 Train Acc 0.9296875 Valid Acc 0.8725\n",
      "Elapsed Time 57.282845974\n",
      "Loss at step:  1850 0.292053 Train Acc 0.8984375 Valid Acc 0.8785\n",
      "Elapsed Time 58.7577750683\n",
      "Loss at step:  1900 0.372558 Train Acc 0.8984375 Valid Acc 0.876\n",
      "Elapsed Time 60.210214138\n",
      "Loss at step:  1950 0.573862 Train Acc 0.8828125 Valid Acc 0.8736\n",
      "Elapsed Time 61.7193441391\n",
      "Loss at step:  2000 0.333659 Train Acc 0.8359375 Valid Acc 0.8803\n",
      "Elapsed Time 63.1094141006\n",
      "Loss at step:  2050 0.255908 Train Acc 0.8828125 Valid Acc 0.879\n",
      "Elapsed Time 64.5888030529\n",
      "Loss at step:  2100 0.355972 Train Acc 0.9296875 Valid Acc 0.8718\n",
      "Elapsed Time 66.1331861019\n",
      "Loss at step:  2150 0.390895 Train Acc 0.9140625 Valid Acc 0.879\n",
      "Elapsed Time 67.7614049911\n",
      "Loss at step:  2200 0.321371 Train Acc 0.8671875 Valid Acc 0.881\n",
      "Elapsed Time 69.6565461159\n",
      "Loss at step:  2250 0.22941 Train Acc 0.921875 Valid Acc 0.8759\n",
      "Elapsed Time 71.5146770477\n",
      "Loss at step:  2300 0.357316 Train Acc 0.921875 Valid Acc 0.8841\n",
      "Elapsed Time 73.9252669811\n",
      "Loss at step:  2350 0.309869 Train Acc 0.8984375 Valid Acc 0.8813\n",
      "Elapsed Time 75.8024740219\n",
      "Loss at step:  2400 0.30667 Train Acc 0.921875 Valid Acc 0.8757\n",
      "Elapsed Time 77.2642021179\n",
      "Loss at step:  2450 0.388781 Train Acc 0.90625 Valid Acc 0.8747\n",
      "Elapsed Time 79.8650481701\n",
      "Loss at step:  2500 0.297296 Train Acc 0.890625 Valid Acc 0.8811\n",
      "Elapsed Time 82.5933430195\n",
      "Loss at step:  2550 0.37248 Train Acc 0.9296875 Valid Acc 0.8779\n",
      "Elapsed Time 85.4443230629\n",
      "Loss at step:  2600 0.419099 Train Acc 0.890625 Valid Acc 0.8741\n",
      "Elapsed Time 87.190472126\n",
      "Loss at step:  2650 0.210669 Train Acc 0.8828125 Valid Acc 0.879\n",
      "Elapsed Time 88.5836830139\n",
      "Loss at step:  2700 0.491134 Train Acc 0.9375 Valid Acc 0.8811\n",
      "Elapsed Time 90.0047700405\n",
      "Loss at step:  2750 0.289116 Train Acc 0.875 Valid Acc 0.8768\n",
      "Elapsed Time 91.3864331245\n",
      "Loss at step:  2800 0.32478 Train Acc 0.9140625 Valid Acc 0.88\n",
      "Elapsed Time 92.7580909729\n",
      "Loss at step:  2850 0.317925 Train Acc 0.8984375 Valid Acc 0.8813\n",
      "Elapsed Time 94.1527509689\n",
      "Loss at step:  2900 0.329158 Train Acc 0.8828125 Valid Acc 0.8818\n",
      "Elapsed Time 95.5336110592\n",
      "Loss at step:  2950 0.508116 Train Acc 0.8984375 Valid Acc 0.8784\n",
      "Elapsed Time 96.9356641769\n",
      "Loss at step:  3000 0.185023 Train Acc 0.90625 Valid Acc 0.8833\n",
      "Elapsed Time 98.367069006\n",
      "Loss at step:  3050 0.389431 Train Acc 0.953125 Valid Acc 0.877\n",
      "Elapsed Time 100.429282188\n",
      "Loss at step:  3100 0.285724 Train Acc 0.8984375 Valid Acc 0.8766\n",
      "Elapsed Time 102.754281998\n",
      "Loss at step:  3150 0.294242 Train Acc 0.921875 Valid Acc 0.8828\n",
      "Elapsed Time 105.525117159\n",
      "Loss at step:  3200 0.313344 Train Acc 0.921875 Valid Acc 0.8812\n",
      "Elapsed Time 107.691642046\n",
      "Loss at step:  3250 0.412211 Train Acc 0.921875 Valid Acc 0.8821\n",
      "Elapsed Time 109.347282171\n",
      "Loss at step:  3300 0.178325 Train Acc 0.8984375 Valid Acc 0.8867\n",
      "Elapsed Time 111.027439117\n",
      "Loss at step:  3350 0.432469 Train Acc 0.9375 Valid Acc 0.8851\n",
      "Elapsed Time 112.624490023\n",
      "Loss at step:  3400 0.309527 Train Acc 0.8984375 Valid Acc 0.8815\n",
      "Elapsed Time 114.240269184\n",
      "Loss at step:  3450 0.433395 Train Acc 0.921875 Valid Acc 0.8839\n",
      "Elapsed Time 115.820300102\n",
      "Test Accuracy 0.9374\n",
      "Runtime 118.292801142\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "minibatch_graph = tf.Graph()\n",
    "with minibatch_graph.as_default():\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (None, IMG_PIXELS))\n",
    "    tf_train_labels  = tf.placeholder(tf.float32, shape = (None, NUM_CLASSES))\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_val_dataset = tf.constant(val_dataset)\n",
    "    \n",
    "    #weights matrix\n",
    "    #initialized as a set of random results \n",
    "    #train weights to predict the log probability for each of the clases based on the\n",
    "    #for each class - sigmoid(w1*x1 + w2*x2 + .... + wN * xN + b)\n",
    "    \n",
    "    w1 = tf.get_variable(\"W1\", shape = (IMG_PIXELS, 1024), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.truncated_normal(shape = [1024], mean = 0.1, stddev=0.05))\n",
    "    #w1 = tf.Variable(tf.truncated_normal( shape = (IMG_PIXELS, 1024)))\n",
    "    #b1 = tf.Variable(tf.zeros([1024]))\n",
    "    h1 = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1)\n",
    "    \n",
    "    w2 = tf.get_variable(\"W2\", shape = (1024, NUM_CLASSES), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.truncated_normal(shape = [NUM_CLASSES], mean = 0.1, stddev=0.05))\n",
    "    #w2 = tf.Variable(tf.truncated_normal( shape = (1024, NUM_CLASSES)))\n",
    "    #b2 = tf.Variable(tf.zeros([NUM_CLASSES]))\n",
    "    logits = tf.matmul(h1, w2) + b2\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(labels = tf_train_labels, logits = logits)) \n",
    "    opt = tf.train.AdamOptimizer().minimize(loss)\n",
    "    \n",
    "    predictions = tf.nn.softmax(logits)\n",
    "    \n",
    "\n",
    "training_steps = 3500\n",
    "train_dataset_size = train_dataset.shape[0]\n",
    "\n",
    "training_hist = []\n",
    "validation_hist = []\n",
    "loss_hist = []\n",
    "start = time.time()\n",
    "with tf.Session(graph = minibatch_graph) as sess:\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    for step in range(training_steps):\n",
    "        sample = np.random.choice(np.arange(train_dataset_size), 128)\n",
    "        data_batch = train_dataset[sample]\n",
    "        label_batch = train_labels[sample]\n",
    "        _, loss_score, pred = sess.run([opt, loss, predictions], feed_dict = {tf_train_dataset : data_batch,\n",
    "                                                                                    tf_train_labels  : label_batch\n",
    "                                                                                   })\n",
    "        if step % 50 == 0:\n",
    "            print 'Loss at step: ', step, loss_score,\n",
    "            print 'Train Acc', train_acc, #biased\n",
    "            print 'Valid Acc', val_acc\n",
    "            print 'Elapsed Time', time.time() - start\n",
    "            \n",
    "            #log results\n",
    "            valid_pred = sess.run(predictions, feed_dict = {tf_train_dataset : val_dataset})\n",
    "            train_acc = acc(pred, label_batch)\n",
    "            val_acc = acc(valid_pred, val_labels)\n",
    "            training_hist.append(train_acc)\n",
    "            validation_hist.append(val_acc)\n",
    "            loss_hist.append(loss_score)\n",
    "        \n",
    "    test_pred = sess.run(predictions, feed_dict = {tf_train_dataset : test_dataset})    \n",
    "    print 'Test Accuracy', acc(test_pred, test_labels)\n",
    "print 'Runtime', time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
